{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "10tOJXQckjvdqAfinmbvARLY3UjqU0WS1",
      "authorship_tag": "ABX9TyOAMI0dMxFGktb8hekbHv3F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alesordo/LabAI/blob/main/LabAI2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "# TensorFlow and tf.keras\n",
        "from tensorflow import keras\n",
        "from keras.applications import VGG16\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import img_to_array\n",
        "from keras.utils import load_img\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imutils import paths\n",
        "\n",
        "# Commonly used modules\n",
        "import statistics\n",
        "import pathlib\n",
        "\n",
        "# Images, plots, display, and visualization\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import cv2\n",
        "import csv\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rIDzU7ksg0vf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "00t6GtnkgBqN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Configuration of directories and variables\n",
        "\n",
        "# Mounting images directories\n",
        "BASE_IN = \"/content/drive/MyDrive/Archive\"\n",
        "TRAIN_DIR = os.path.sep.join([BASE_IN, \"Train\"])\n",
        "TEST_DIR = os.path.sep.join([BASE_IN + \"Test\"])\n",
        "PATHS_VAL = os.path.sep.join([BASE_IN + \"ValidationPaths.csv\"])\n",
        "ANNOTATION_TRAIN = os.path.sep.join([BASE_IN, \"Train.csv\"])\n",
        "ANNOTATION_TEST = os.path.sep.join([BASE_IN, \"Test.csv\"])\n",
        "\n",
        "#Mounting output directories\n",
        "BASE_OUT = os.path.sep.join([BASE_IN, \"Output\"])\n",
        "MODEL_PATH = os.path.sep.join([BASE_OUT, \"model.h5\"])\n",
        "LB_PATH = os.path.sep.join([BASE_OUT, \"lb.pickle\"])\n",
        "PLOTS_PATH = os.path.sep.join([BASE_OUT, \"plots\"])\n",
        "\n",
        "# Fixed variables\n",
        "WIDTH = 50\n",
        "HEIGHT = 50\n",
        "N_CLASSES = 43\n",
        "\n",
        "#Deep learning hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "N_CHANNELS = 3\n",
        "INIT_LR = 1e-4\n",
        "NUM_EPOCHS = 15\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Loading datasets of images, labels, bounding boxes and images paths\n",
        "\n",
        "#Initialize list of images, class label, bounding box coordinates, image paths\n",
        "print(\"Loading datasets...\")\n",
        "data = []\n",
        "labels = []\n",
        "bboxes = []\n",
        "imagePaths = []\n",
        "\n",
        "#Load annotation file\n",
        "with open(ANNOTATION_TRAIN, \"r\") as csvfile:\n",
        "    rows = csv.reader(csvfile)\n",
        "    next(rows)\n",
        "    #Loop rows\n",
        "    for row in tqdm(rows):\n",
        "        #Obtain each data from the csv\n",
        "        (w, h, startX, startY, endX, endY, label, relativeFilePath) = row\n",
        "\n",
        "        #Reading complete filepaths and images in OpenCV format\n",
        "        imagePath = os.path.sep.join([BASE_IN, relativeFilePath])\n",
        "        image = cv2.imread(imagePath)\n",
        "\n",
        "        # scale the bounding box coordinates relative to the spatial\n",
        "        # dimensions of the input image\n",
        "        startX = float(startX) / float(w)\n",
        "        startY = float(startY) / float(h)\n",
        "        endX = float(endX) / float(w)\n",
        "        endY = float(endY) / float(h)\n",
        "\n",
        "        # load the image and preprocess it\n",
        "        image = load_img(imagePath, target_size=(224, 224))\n",
        "        image = img_to_array(image)\n",
        "\n",
        "        #debug for future\n",
        "        # plt.imshow(image.astype(np.uint8))\n",
        "        # plt.savefig(BASE_IN+\"/img.jpg\")\n",
        "\n",
        "        # update our list of data, class labels, bounding boxes, and\n",
        "        # image paths\n",
        "        data.append(image)\n",
        "        labels.append(label)\n",
        "        bboxes.append((startX, startY, endX, endY))\n",
        "        imagePaths.append(imagePath)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "q3VXzi0PiZ0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title NEW Loading dataset of labels, bounding boxes and images paths\n",
        "\n",
        "#Initialize list of images, class label, bounding box coordinates, image paths\n",
        "print(\"Loading datasets...\")\n",
        "data = []\n",
        "labels = []\n",
        "bboxes = []\n",
        "imagePaths = []\n",
        "\n",
        "#Load annotation file\n",
        "with open(ANNOTATION_TRAIN, \"r\") as csvfile:\n",
        "    rows = csv.reader(csvfile)\n",
        "    next(rows)\n",
        "    #Loop rows\n",
        "    for row in tqdm(rows):\n",
        "        #Obtain each data from the csv\n",
        "        (w, h, startX, startY, endX, endY, label, relativeFilePath) = row\n",
        "\n",
        "        #Reading complete filepaths and images in OpenCV format\n",
        "        imagePath = os.path.sep.join([BASE_IN, relativeFilePath])\n",
        "\n",
        "        # scale the bounding box coordinates relative to the spatial\n",
        "        # dimensions of the input image\n",
        "        startX = float(startX) / float(w)\n",
        "        startY = float(startY) / float(h)\n",
        "        endX = float(endX) / float(w)\n",
        "        endY = float(endY) / float(h)\n",
        "\n",
        "        #debug for future\n",
        "        # plt.imshow(image.astype(np.uint8))\n",
        "        # plt.savefig(BASE_IN+\"/img.jpg\")\n",
        "\n",
        "        # update our list of data, class labels, bounding boxes, and\n",
        "        # image paths\n",
        "        labels.append(label)\n",
        "        bboxes.append((startX, startY, endX, endY))\n",
        "        imagePaths.append(imagePath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "TvMHhTHcmuw5",
        "outputId": "b54bbc7a-2bcd-4abb-8e58-4a2abe44f74d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "39209it [00:00, 211111.34it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title NEW Loading dataset of images\n",
        "\n",
        "NUM_CATEGORIES = len(os.listdir(TRAIN_DIR))\n",
        "\n",
        "for img in tqdm(imagePaths):\n",
        "  try:\n",
        "      image = cv2.imread(img)\n",
        "      image_fromarray = Image.fromarray(image, 'RGB')\n",
        "      resize_image = image_fromarray.resize((HEIGHT, WIDTH))\n",
        "      data.append(np.array(resize_image))\n",
        "  except:\n",
        "      print(\"Error in \" + img)\n",
        "\n",
        "data = np.array(data, dtype=\"float32\") / 255.0\n",
        "# Changing the list to numpy array\n",
        "#data = np.array(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "-ETMMi5hkL6l",
        "outputId": "627eb66c-4cb4-4fcb-9b84-86e0a733537c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|â–Ž         | 1370/39209 [07:40<3:10:33,  3.31it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Further preprocessing\n",
        "\n",
        "# convert the data, class labels, bounding boxes, and image paths to NumPy arrays, scaling the input pixel from the range [0, 255] to [0, 1]\n",
        "#data = np.array(data, dtype=\"float32\") / 255.0\n",
        "labels = np.array(labels)\n",
        "bboxes = np.array(bboxes, dtype=\"float32\")\n",
        "imagePaths = np.array(imagePaths)\n",
        "\n",
        "# perform one-hot encoding on the labels\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JmYL-y53qDwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Partition data into training and validation - EXECUTE ONCE, NOT BECAUSE IT CHANGES FILES, BECAUSE IT SAVES VALIDATION PATHS\n",
        "\n",
        "# partition the data into training and testing splits using 80% of the data for training and the remaining 20% for testing\n",
        "split = train_test_split(data, labels, bboxes, imagePaths,\n",
        "\ttest_size=0.20, random_state=42)\n",
        "\n",
        "# unpack the data split\n",
        "(trainImages, valImages) = split[:2]\n",
        "(trainLabels, valLabels) = split[2:4]\n",
        "(trainBBoxes, valBBoxes) = split[4:6]\n",
        "(trainPaths, valPaths) = split[6:]\n",
        "\n",
        "# write the testing image paths to disk so that we can use then when evaluating/testing our object detector\n",
        "print(\"[INFO] saving testing image paths...\")\n",
        "f = open(PATHS_VAL, \"w\")\n",
        "f.write(\"\\n\".join(valPaths))\n",
        "f.close()"
      ],
      "metadata": {
        "id": "Y2dZOiVQqXjJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize VGG16\n",
        "\n",
        "# load the VGG16 network, ensuring the head FC layers are left off\n",
        "vgg = VGG16(weights=\"imagenet\", include_top=False,\n",
        "\tinput_tensor=Input(shape=(HEIGHT, WIDTH, 3)))\n",
        "\n",
        "# freeze all VGG layers so they will *not* be updated during the training process\n",
        "vgg.trainable = False\n",
        "\n",
        "# flatten the max-pooling output of VGG\n",
        "flatten = vgg.output\n",
        "flatten = Flatten()(flatten)"
      ],
      "metadata": {
        "id": "cC1ECRjl7kKB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Constructing layer head with two branches\n",
        "\n",
        "# construct a fully-connected layer header to output the predicted bounding box coordinates\n",
        "bboxHead = Dense(128, activation=\"relu\")(flatten)\n",
        "bboxHead = Dense(64, activation=\"relu\")(bboxHead)\n",
        "bboxHead = Dense(32, activation=\"relu\")(bboxHead)\n",
        "bboxHead = Dense(4, activation=\"sigmoid\",\n",
        "\tname=\"bounding_box\")(bboxHead)\n",
        " \n",
        "# construct a second fully-connected layer head, this one to predict the class label\n",
        "softmaxHead = Dense(512, activation=\"relu\")(flatten)\n",
        "softmaxHead = Dropout(0.5)(softmaxHead)\n",
        "softmaxHead = Dense(512, activation=\"relu\")(softmaxHead)\n",
        "softmaxHead = Dropout(0.5)(softmaxHead)\n",
        "softmaxHead = Dense(len(lb.classes_), activation=\"softmax\",\n",
        "\tname=\"class_label\")(softmaxHead)\n",
        " \n",
        "# put together our model which accept an input image and then output bounding box coordinates and a class label\n",
        "model = Model(\n",
        "\tinputs=vgg.input,\n",
        "\toutputs=(bboxHead, softmaxHead))"
      ],
      "metadata": {
        "id": "z1YiRZ4774Tr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Defining dictionaries for losses and compilation of the model\n",
        "\n",
        "# define a dictionary to set the loss methods -- categorical cross-entropy for the class label head and mean absolute error for the bounding box head\n",
        "losses = {\n",
        "\t\"class_label\": \"categorical_crossentropy\",\n",
        "\t\"bounding_box\": \"mean_squared_error\",\n",
        "}\n",
        "\n",
        "# define a dictionary that specifies the weights per loss (both the class label and bounding box outputs will receive equal weight)\n",
        "lossWeights = {\n",
        "\t\"class_label\": 1.0,\n",
        "\t\"bounding_box\": 1.0\n",
        "}\n",
        "\n",
        "# initialize the optimizer, compile the model, and show the model summary\n",
        "opt = Adam(lr=INIT_LR)\n",
        "model.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=lossWeights)\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "RyRPlezy8w1_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Defining dictionaries that represent training and validation set\n",
        "\n",
        "# construct a dictionary for our target training outputs\n",
        "trainTargets = {\n",
        "\t\"class_label\": trainLabels,\n",
        "\t\"bounding_box\": trainBBoxes\n",
        "}\n",
        "# construct a second dictionary, this one for our target testing\n",
        "# outputs\n",
        "valTargets = {\n",
        "\t\"class_label\": valLabels,\n",
        "\t\"bounding_box\": valBBoxes\n",
        "}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "u7lmtbKT9Ag_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training network\n",
        "\n",
        "# train the network for bounding box regression and class label prediction\n",
        "print(\"[INFO] training model...\")\n",
        "H = model.fit(\n",
        "\ttrainImages, trainTargets,\n",
        "\tvalidation_data=(valImages, valTargets),\n",
        "\tbatch_size=BATCH_SIZE,\n",
        "\tepochs=NUM_EPOCHS,\n",
        "\tverbose=1)\n",
        "\n",
        "# serialize the model to disk\n",
        "print(\"[INFO] saving object detector model...\")\n",
        "model.save(MODEL_PATH, save_format=\"h5\")\n",
        "\n",
        "# serialize the label binarizer to disk\n",
        "print(\"[INFO] saving label binarizer...\")\n",
        "f = open(LB_PATH, \"wb\")\n",
        "f.write(pickle.dumps(lb))\n",
        "f.close()"
      ],
      "metadata": {
        "id": "KRMhVlSJ_IN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plotting losses and accuracy\n",
        "\n",
        "# plot the total loss, label loss, and bounding box loss\n",
        "lossNames = [\"loss\", \"class_label_loss\", \"bounding_box_loss\"]\n",
        "N = np.arange(0, NUM_EPOCHS)\n",
        "plt.style.use(\"ggplot\")\n",
        "(fig, ax) = plt.subplots(3, 1, figsize=(13, 13))\n",
        "\n",
        "# loop over the loss names\n",
        "for (i, l) in enumerate(lossNames):\n",
        "\t# plot the loss for both the training and validation data\n",
        "\ttitle = \"Loss for {}\".format(l) if l != \"loss\" else \"Total loss\"\n",
        "\tax[i].set_title(title)\n",
        "\tax[i].set_xlabel(\"Epoch #\")\n",
        "\tax[i].set_ylabel(\"Loss\")\n",
        "\tax[i].plot(N, H.history[l], label=l)\n",
        "\tax[i].plot(N, H.history[\"val_\" + l], label=\"val_\" + l)\n",
        "\tax[i].legend()\n",
        " \n",
        "# save the losses figure and create a new figure for the accuracies\n",
        "plt.tight_layout()\n",
        "plotPath = os.path.sep.join([PLOTS_PATH, \"losses.png\"])\n",
        "plt.savefig(plotPath)\n",
        "plt.close()\n",
        "\n",
        "# create a new figure for the accuracies\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"class_label_accuracy\"],\n",
        "\tlabel=\"class_label_train_acc\")\n",
        "plt.plot(N, H.history[\"val_class_label_accuracy\"],\n",
        "\tlabel=\"val_class_label_acc\")\n",
        "plt.title(\"Class Label Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "\n",
        "# save the accuracies plot\n",
        "plotPath = os.path.sep.join([PLOTS_PATH, \"accs.png\"])\n",
        "plt.savefig(plotPath)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uKi52jAbAHDe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}